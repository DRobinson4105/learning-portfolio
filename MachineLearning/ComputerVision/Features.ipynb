{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "Examples of Using Features\n",
    "1. Matching - Extract features from two different images to compare them rather than pixel comparison\n",
    "2. Structure from Motion - Use features from many images to construct something that has been identified from those images from different viewpoints\n",
    "\n",
    "## Interest Points\n",
    "Points in an image that can be detected and are relevant for higher level processing\n",
    "Often used in applications like image stabilization and structure from motion\n",
    "Applications\n",
    "    - Key-Point Matching\n",
    "        1. Find a set of distinctive key-points\n",
    "        2. Define a region around each key-point\n",
    "        3. Extract and normalize the region content\n",
    "        4. Compute a local descriptor from the normalized region\n",
    "        5. Match local descriptors\n",
    "### Possible Approaches\n",
    "Corner Detection\n",
    "- Recognize corners by looking at a small region.\n",
    "    - \"Flat\" if no change in intensity in all directions\n",
    "    - \"Edge\" if no change in intensity along edge direction\n",
    "    - \"Corner\" if significant change in intensity in all directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torchvision.transforms.functional as cvF\n",
    "from skimage.draw import line\n",
    "from math import *\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"img2.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "image = transform(img).to(device).unsqueeze(0)\n",
    "img = transforms.ToPILImage()(image.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harris Corner Detection\n",
    "$$E(u, v)=\\sum_{x,y}w(x, y)[I(x+u, y+v)-I(x, y)]^2$$\n",
    "1. Compute $M$ matrix for each window to recover a *cornerness* score $C$.\n",
    "2. Threshold to find pixels which give large corner response\n",
    "3. Find the local maximima pixels (non-maximum suppresion)\n",
    "\n",
    "Steps:\n",
    "1. Compute image gradients\n",
    "2. Compute $M$ components as squares of derivatives\n",
    "3. Gaussian filter with width $s$: $g(I_x^2), g(I_y^2), g(I_x\\circ I_y)$\n",
    "4. Compute cornerness $C = det(M)-\\alpha trace(M)^2=g(I_x^2)\\circ g(I_y^2)-g(I_x\\circ I_y)^2-\\alpha[g(I_x^2)+g(I_y^2)]^2$\n",
    "5. Threshold on $C$ to filter for high cornerness\n",
    "6. Non-maximal suppression to pick peak corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(image):\n",
    "    kernel_x = torch.tensor(\n",
    "        [[1, 0, -1], [2, 0, -2], [1, 0, -1]], \n",
    "        dtype=torch.float, device=device\n",
    "    ).view(1, 1, 3, 3)\n",
    "    kernel_y = torch.tensor(\n",
    "        [[1, 2, 1], [0, 0, 0], [-1, -2, -1]],\n",
    "        dtype=torch.float, device=device\n",
    "    ).view(1, 1, 3, 3)\n",
    "\n",
    "    # Compute gradients in x and y direction\n",
    "    grad_x = F.conv2d(image, kernel_x, padding=1)\n",
    "    grad_y = F.conv2d(image, kernel_y, padding=1)\n",
    "    \n",
    "    return grad_x, grad_y\n",
    "\n",
    "def non_max_suppression(C, corners, radius):\n",
    "    dilated = F.max_pool2d(C, kernel_size=(radius*2+1, radius*2+1), stride=1, padding=radius)\n",
    "    local_max = (C == dilated.squeeze())\n",
    "    return corners & local_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.04\n",
    "threshold = 0.01\n",
    "\n",
    "# Compute image gradients\n",
    "grad_x, grad_y = compute_gradients(image)\n",
    "\n",
    "# Compute products of gradients\n",
    "Ixx = grad_x ** 2\n",
    "Iyy = grad_y ** 2\n",
    "Ixy = grad_x * grad_y\n",
    "\n",
    "# Smooth products of gradients with gaussian filter\n",
    "Ixx = cvF.gaussian_blur(Ixx, 5, 1.5)\n",
    "Iyy = cvF.gaussian_blur(Iyy, 5, 1.5)\n",
    "Ixy = cvF.gaussian_blur(Ixy, 5, 1.5)\n",
    "\n",
    "# Compute cornerness\n",
    "det_M = Ixx * Iyy - Ixy ** 2\n",
    "trace_M = Ixx + Iyy\n",
    "C = det_M - alpha * (trace_M ** 2)\n",
    "\n",
    "# Threshold on C to filter for high cornerness\n",
    "corners = C > threshold * C.max()\n",
    "\n",
    "corners = non_max_suppression(C, corners, 3)\n",
    "\n",
    "# Mark corners\n",
    "corner_image = image.cpu().numpy()\n",
    "corner_image[corners.cpu()] = 255\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Harris Corner Detection\")\n",
    "plt.imshow(corner_image.squeeze(), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms of Oriented Gradients\n",
    "1. Extract a square window (block) of some size\n",
    "2. Divide block into a square grid of sub-blocks (cells)\n",
    "3. Compute orientation histogram of each cell\n",
    "4. Concatenate the histograms together\n",
    "5. Normalize $v$ (the concatenation of the histograms)\n",
    "    - Option 1: Divide $v$ by its Euclidean norm\n",
    "    - Option 2: Divide $v$ by its $L_1$ norm (sum of all absolute values of $v$)\n",
    "    - Option 3:\n",
    "        - Divide $v$ by its Euclidean norm\n",
    "        - Clip any value over 0.2\n",
    "        - Divide the resulting $v$ by its Euclidean norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histograms(grad, theta, n_bins=9):\n",
    "    bins = torch.linspace(0, 180, n_bins + 1, device=device)\n",
    "    histograms = torch.zeros((32, 32, n_bins), dtype=torch.float, device=device)\n",
    "\n",
    "    grad_magnitudes = grad.view(1, 1, 32, 8, 32, 8)\n",
    "    grad_orientations = theta.view(1, 1, 32, 8, 32, 8)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        lower_bound = bins[i]\n",
    "        upper_bound = bins[i + 1]\n",
    "        mask = (grad_orientations >= lower_bound) & (grad_orientations < upper_bound)\n",
    "        masked_magnitudes = grad_magnitudes * mask.float()\n",
    "\n",
    "        histograms[:, :, i] = masked_magnitudes.sum(dim=[3, 5])\n",
    "\n",
    "    return histograms\n",
    "\n",
    "def hog(histograms, cell_len=8, n_bins=9):\n",
    "    n_cells = histograms.shape[0]\n",
    "    image_len = n_cells * cell_len\n",
    "\n",
    "    radius = min(cell_len, cell_len) // 2 - 1\n",
    "    orientations_arr = torch.arange(n_bins)\n",
    "\n",
    "    orientation_bin_midpoints = torch.pi * (orientations_arr + 0.5) / n_bins\n",
    "    dr_arr = radius * torch.sin(orientation_bin_midpoints)\n",
    "    dc_arr = radius * torch.cos(orientation_bin_midpoints)\n",
    "    hog_image = torch.zeros((image_len, image_len), dtype=torch.float)\n",
    "\n",
    "    for r in range(n_cells):\n",
    "        for c in range(n_cells):\n",
    "            for o, dr, dc in zip(orientations_arr, dr_arr, dc_arr):\n",
    "                center = tuple([r * cell_len + cell_len // 2, c * cell_len + cell_len // 2])\n",
    "                rr, cc = line(\n",
    "                    int(center[0] - dc),\n",
    "                    int(center[1] + dr),\n",
    "                    int(center[0] + dc),\n",
    "                    int(center[1] - dr),\n",
    "                )\n",
    "                hog_image[rr, cc] += histograms[r, c, o]\n",
    "\n",
    "    return hog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"img1.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "image = transform(img).to(device).unsqueeze(0)\n",
    "img = transforms.ToPILImage()(image.squeeze())\n",
    "\n",
    "# Compute gradients\n",
    "grad_x, grad_y = compute_gradients(image)\n",
    "\n",
    "# Compute gradient magnitude and orientation\n",
    "grad = torch.sqrt(grad_x ** 2 + grad_y ** 2)\n",
    "theta = torch.atan2(grad_y, grad_x)\n",
    "\n",
    "grad = grad / grad.max()\n",
    "\n",
    "# Normalize theta to range [0, 180)\n",
    "theta = theta * 180.0 / torch.pi\n",
    "theta[theta < 0] += 180\n",
    "theta = torch.remainder(theta, 180.0)\n",
    "\n",
    "histograms = get_histograms(grad, theta)\n",
    "\n",
    "hog_image = hog(histograms.cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Histogram of Oriented Gradients\")\n",
    "plt.imshow(hog_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Invariant Feature Transform (SIFT)\n",
    "Uses image to identify local feature coordinates that is invariant to translation, rotation, or scale\n",
    "\n",
    "1. Scale-Space Extrema Detection - Search over multiple scales and image locations\n",
    "    - Finding blobs by approximating LoG with DoG\n",
    "    - Obtain gaussian pyramid for each layer in an octave blur with different sigms for DoG\n",
    "2. Key-Point Localization - Fit a model to determine location and scale, then select key-points based on a measure of stability\n",
    "3. Orientation Assignment - Compute best orientations for each key-point region\n",
    "4. Key-Point Description - Use local image gradients at selected scale and rotation to describe each key-point region\n",
    "\n",
    "Automatic Scale Selection\n",
    "- Function responses for increasing scale (map different scales to the function results and the highest function result has the best scale)\n",
    "- Function can be 2nd derivative of Gaussian (Laplacian of Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT.create()\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "img1 = cv2.imread(\"img3.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.resize(img1, (1024, 1024))\n",
    "img2 = cv2.imread(\"img3.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.resize(img2, (1024, 1024))\n",
    "\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(img1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "img3 = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[300:600], img2)\n",
    "\n",
    "plt.imshow(img3, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_space_extrema_detection(image: torch.Tensor, num_octaves=4, num_scales=5, sigma=1.6):\n",
    "    octaves = []\n",
    "    k = 2 ** (1 / num_scales)\n",
    "\n",
    "    for _ in range(num_octaves):\n",
    "        _, height, width = image.shape\n",
    "        octave = []\n",
    "\n",
    "        for scale in range(1, num_scales+2):\n",
    "            c_sigma = sigma * (k ** scale)\n",
    "            c_kernel_size = 2 * ceil(3 * c_sigma) + 1\n",
    "\n",
    "            scaled = cvF.gaussian_blur(image, c_kernel_size, c_sigma)\n",
    "\n",
    "            octave.append(scaled)\n",
    "\n",
    "        dog_octave = torch.zeros((num_scales, height, width), dtype=torch.float, device=device)\n",
    "\n",
    "        for idx in range(1, num_scales+1):\n",
    "            dog_octave[idx-1] = octave[idx] - octave[idx-1]\n",
    "            \n",
    "        octaves.append(dog_octave)\n",
    "        image = F.avg_pool2d(image, 2)\n",
    "\n",
    "    return octaves\n",
    "\n",
    "def detect_keypoints(octaves, contrast_threshold=0.05):\n",
    "    keypoints = []\n",
    "    for o, octave in enumerate(octaves):\n",
    "        neg_tensor = F.pad(octave, (1, 1, 1, 1, 1, 1), mode='constant', value=float('-inf'))\n",
    "        pos_tensor = F.pad(octave, (1, 1, 1, 1, 1, 1), mode='constant', value=float('inf'))\n",
    "\n",
    "        neighbors = lambda padded_tensor: torch.stack([\n",
    "            padded_tensor[:-2, :-2, :-2],  padded_tensor[:-2, :-2, 1:-1],  padded_tensor[:-2, :-2, 2:],\n",
    "            padded_tensor[:-2, 1:-1, :-2], padded_tensor[:-2, 1:-1, 1:-1], padded_tensor[:-2, 1:-1, 2:],\n",
    "            padded_tensor[:-2, 2:, :-2],   padded_tensor[:-2, 2:, 1:-1],   padded_tensor[:-2, 2:, 2:],\n",
    "\n",
    "            padded_tensor[1:-1, :-2, :-2], padded_tensor[1:-1, :-2, 1:-1], padded_tensor[1:-1, :-2, 2:],\n",
    "            padded_tensor[1:-1, 1:-1, :-2],                                padded_tensor[1:-1, 1:-1, 2:],\n",
    "            padded_tensor[1:-1, 2:, :-2],  padded_tensor[1:-1, 2:, 1:-1],  padded_tensor[1:-1, 2:, 2:],\n",
    "\n",
    "            padded_tensor[2:, :-2, :-2],   padded_tensor[2:, :-2, 1:-1],   padded_tensor[2:, :-2, 2:],\n",
    "            padded_tensor[2:, 1:-1, :-2],  padded_tensor[2:, 1:-1, 1:-1],  padded_tensor[2:, 1:-1, 2:],\n",
    "            padded_tensor[2:, 2:, :-2],    padded_tensor[2:, 2:, 1:-1],    padded_tensor[2:, 2:, 2:]\n",
    "        ])\n",
    "\n",
    "        # Compare the central tensor with all its neighbors to find local maximas\n",
    "        center_tensor = neg_tensor[1:-1, 1:-1, 1:-1]\n",
    "        comparison = center_tensor.unsqueeze(0) > (neighbors(neg_tensor) + contrast_threshold)\n",
    "\n",
    "        # Check if each element is greater than all its neighbors\n",
    "        result_max = torch.all(comparison, dim=0).nonzero()\n",
    "        result_max = torch.cat((torch.full((result_max.shape[0], 1), o), result_max), dim=1)\n",
    "\n",
    "        # Compare the central tensor with all its neighbors to find local minimas\n",
    "        center_tensor = pos_tensor[1:-1, 1:-1, 1:-1]\n",
    "        comparison = center_tensor.unsqueeze(0) < (neighbors(pos_tensor) - contrast_threshold)\n",
    "\n",
    "        # Check if each element is less than all its neighbors\n",
    "        result_min = torch.all(comparison, dim=0).nonzero()\n",
    "        result_min = torch.cat((torch.full((result_min.shape[0], 1), o), result_min), dim=1)\n",
    "\n",
    "        keypoints.append(torch.cat((result_min, result_max)))\n",
    "        \n",
    "    keypoints = torch.cat(keypoints)\n",
    "    return keypoints\n",
    "\n",
    "def assign_orientation(keypoints, octaves, sigma):\n",
    "    oriented_keypoints = []\n",
    "    for o, s, y, x in keypoints:\n",
    "        region = octaves[o][s][y-8:y+8, x-8:x+8]\n",
    "\n",
    "        if region.shape != (16, 16):\n",
    "            continue\n",
    "\n",
    "        gradient_magnitude = torch.sqrt(F.conv2d(F.pad(region, (1, 1)).unsqueeze(0).unsqueeze(0), torch.tensor([[-1, 0, 1]]).float().unsqueeze(0).unsqueeze(0)) ** 2 + \n",
    "                                        F.conv2d(F.pad(region, (0, 0, 1, 1)).unsqueeze(0).unsqueeze(0), torch.tensor([[-1], [0], [1]]).float().unsqueeze(0).unsqueeze(0)) ** 2)\n",
    "        gradient_orientation = torch.atan2(F.conv2d(F.pad(region, (0, 0, 1, 1)).unsqueeze(0).unsqueeze(0), torch.tensor([[-1], [0], [1]]).float().unsqueeze(0).unsqueeze(0)), \n",
    "                                           F.conv2d(F.pad(region, (1, 1)).unsqueeze(0).unsqueeze(0), torch.tensor([[-1, 0, 1]]).float().unsqueeze(0).unsqueeze(0)))\n",
    "        histogram, _ = torch.histogram(gradient_orientation, bins=36, range=(-np.pi, np.pi), weight=gradient_magnitude)\n",
    "        dominant_orientation = torch.argmax(histogram).item() * 10\n",
    "        oriented_keypoints.append((o.item(), s.item(), y.item(), x.item(), dominant_orientation))\n",
    "    return oriented_keypoints\n",
    "\n",
    "def plot_square(x, y, r, o):\n",
    "    theta = torch.tensor(o * torch.pi / 180.0, device=device, dtype=torch.float)\n",
    "    \n",
    "    corners = torch.tensor([\n",
    "        [-r, -r],\n",
    "        [r, -r],\n",
    "        [r, r],\n",
    "        [-r, r]\n",
    "    ], device=device, dtype=torch.float)\n",
    "    \n",
    "    rotation_matrix = torch.tensor([\n",
    "        [torch.cos(theta), -torch.sin(theta)],\n",
    "        [torch.sin(theta), torch.cos(theta)]\n",
    "    ], device=device, dtype=torch.float)\n",
    "    \n",
    "    rotated_corners = torch.matmul(corners, rotation_matrix)\n",
    "    \n",
    "    translated_corners = rotated_corners + torch.tensor([x, y], device=device, dtype=torch.float)\n",
    "    \n",
    "    square = plt.Polygon(translated_corners.cpu().numpy(), closed=True, edgecolor='red', fill=None)\n",
    "    return square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('img1.png', cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, (512, 512))\n",
    "image = torch.from_numpy(image).to(device=device, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "sigma = 1.6\n",
    "num_octaves = 4\n",
    "num_scales = 5\n",
    "\n",
    "octaves = scale_space_extrema_detection(image)\n",
    "keypoints = detect_keypoints(octaves, contrast_threshold=0.1)\n",
    "\n",
    "image = image.squeeze().cpu().numpy()\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n",
    "for keypoint in keypoints:\n",
    "    o, s, y, x = keypoint.cpu().numpy()\n",
    "\n",
    "    r = (s + 6) * 2 ** o\n",
    "    x *= 2 ** o\n",
    "    y *= 2 ** o\n",
    "    \n",
    "    circle = plt.Circle((x, y), r, edgecolor='r', facecolor='none')\n",
    "    plt.gca().add_patch(circle)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image, cmap='gray')\n",
    "\n",
    "oriented_keypoints = assign_orientation(keypoints, octaves, sigma)\n",
    "\n",
    "for keypoint in oriented_keypoints:\n",
    "    o, s, y, x, ori = keypoint\n",
    "\n",
    "    r = (s + 6) * 2 ** o\n",
    "    x *= 2 ** o\n",
    "    y *= 2 ** o\n",
    "\n",
    "    plt.gca().add_patch(plot_square(x, y, r/2, ori))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
