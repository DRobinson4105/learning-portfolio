{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "Examples of Using Features\n",
    "1. Matching - Extract features from two different images to compare them rather than pixel comparison\n",
    "2. Structure from Motion - Use features from many images to construct something that has been identified from those images from different viewpoints\n",
    "\n",
    "## Interest Points\n",
    "Points in an image that can be detected and are relevant for higher level processing\n",
    "Often used in applications like image stabilization and structure from motion\n",
    "Applications\n",
    "    - Key-Point Matching\n",
    "        1. Find a set of distinctive key-points\n",
    "        2. Define a region around each key-point\n",
    "        3. Extract and normalize the region content\n",
    "        4. Compute a local descriptor from the normalized region\n",
    "        5. Match local descriptors\n",
    "### Possible Approaches\n",
    "Corner Detection\n",
    "- Recognize corners by looking at a small region.\n",
    "    - \"Flat\" if no change in intensity in all directions\n",
    "    - \"Edge\" if no change in intensity along edge direction\n",
    "    - \"Corner\" if significant change in intensity in all directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms.functional as cvF\n",
    "from skimage.feature import hog\n",
    "from skimage.draw import line\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"img2.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "image = transform(img).to(device).unsqueeze(0)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harris Corner Detection\n",
    "$$E(u, v)=\\sum_{x,y}w(x, y)[I(x+u, y+v)-I(x, y)]^2$$\n",
    "1. Compute $M$ matrix for each window to recover a *cornerness* score $C$.\n",
    "2. Threshold to find pixels which give large corner response\n",
    "3. Find the local maximima pixels (non-maximum suppresion)\n",
    "\n",
    "Steps:\n",
    "1. Compute image gradients\n",
    "2. Compute $M$ components as squares of derivatives\n",
    "3. Gaussian filter with width $s$: $g(I_x^2), g(I_y^2), g(I_x\\circ I_y)$\n",
    "4. Compute cornerness $C = det(M)-\\alpha trace(M)^2=g(I_x^2)\\circ g(I_y^2)-g(I_x\\circ I_y)^2-\\alpha[g(I_x^2)+g(I_y^2)]^2$\n",
    "5. Threshold on $C$ to filter for high cornerness\n",
    "6. Non-maximal suppression to pick peak corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(image):\n",
    "    kernel_x = torch.tensor(\n",
    "        [[1, 0, -1], [2, 0, -2], [1, 0, -1]], \n",
    "        dtype=torch.float, device=device\n",
    "    ).view(1, 1, 3, 3)\n",
    "    kernel_y = torch.tensor(\n",
    "        [[1, 2, 1], [0, 0, 0], [-1, -2, -1]],\n",
    "        dtype=torch.float, device=device\n",
    "    ).view(1, 1, 3, 3)\n",
    "\n",
    "    # Compute gradients in x and y direction\n",
    "    grad_x = F.conv2d(image, kernel_x, padding=1)\n",
    "    grad_y = F.conv2d(image, kernel_y, padding=1)\n",
    "    \n",
    "    return grad_x, grad_y\n",
    "\n",
    "def non_max_suppression(C, corners, radius):\n",
    "    dilated = F.max_pool2d(C, kernel_size=(radius*2+1, radius*2+1), stride=1, padding=radius)\n",
    "    local_max = (C == dilated.squeeze())\n",
    "    return corners & local_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.04\n",
    "threshold = 0.01\n",
    "\n",
    "# Compute image gradients\n",
    "grad_x, grad_y = compute_gradients(image)\n",
    "\n",
    "# Compute products of gradients\n",
    "Ixx = grad_x ** 2\n",
    "Iyy = grad_y ** 2\n",
    "Ixy = grad_x * grad_y\n",
    "\n",
    "# Smooth products of gradients with gaussian filter\n",
    "Ixx = cvF.gaussian_blur(Ixx, 5, 1.5)\n",
    "Iyy = cvF.gaussian_blur(Iyy, 5, 1.5)\n",
    "Ixy = cvF.gaussian_blur(Ixy, 5, 1.5)\n",
    "\n",
    "# Compute cornerness\n",
    "det_M = Ixx * Iyy - Ixy ** 2\n",
    "trace_M = Ixx + Iyy\n",
    "C = det_M - alpha * (trace_M ** 2)\n",
    "\n",
    "# Threshold on C to filter for high cornerness\n",
    "corners = C > threshold * C.max()\n",
    "\n",
    "corners = non_max_suppression(C, corners, 3)\n",
    "\n",
    "# Mark corners\n",
    "corner_image = image.cpu().numpy()\n",
    "corner_image[corners.cpu()] = 255\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Harris Corner Detection\")\n",
    "plt.imshow(corner_image.squeeze(), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms of Oriented Gradients\n",
    "1. Extract a square window (block) of some size\n",
    "2. Divide block into a square grid of sub-blocks (cells)\n",
    "3. Compute orientation histogram of each cell\n",
    "4. Concatenate the histograms together\n",
    "5. Normalize $v$ (the concatenation of the histograms)\n",
    "    - Option 1: Divide $v$ by its Euclidean norm\n",
    "    - Option 2: Divide $v$ by its $L_1$ norm (sum of all absolute values of $v$)\n",
    "    - Option 3:\n",
    "        - Divide $v$ by its Euclidean norm\n",
    "        - Clip any value over 0.2\n",
    "        - Divide the resulting $v$ by its Euclidean norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histograms(grad, theta, n_bins=9):\n",
    "    bins = torch.linspace(0, 180, n_bins + 1, device=device)\n",
    "    histograms = torch.zeros((32, 32, n_bins), dtype=torch.float, device=device)\n",
    "\n",
    "    grad_magnitudes = grad.view(1, 1, 32, 8, 32, 8)\n",
    "    grad_orientations = theta.view(1, 1, 32, 8, 32, 8)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        lower_bound = bins[i]\n",
    "        upper_bound = bins[i + 1]\n",
    "        mask = (grad_orientations >= lower_bound) & (grad_orientations < upper_bound)\n",
    "        masked_magnitudes = grad_magnitudes * mask.float()\n",
    "\n",
    "        # Sum magnitudes within each 8x8 block and assign to corresponding bin\n",
    "        histograms[:, :, i] = masked_magnitudes.sum(dim=[3, 5])\n",
    "\n",
    "    return histograms\n",
    "\n",
    "def hog(histograms, cell_len=8, n_bins=9):\n",
    "    n_cells = histograms.shape[0]\n",
    "    image_len = n_cells * cell_len\n",
    "\n",
    "    radius = min(cell_len, cell_len) // 2 - 1\n",
    "    orientations_arr = torch.arange(n_bins)\n",
    "\n",
    "    orientation_bin_midpoints = torch.pi * (orientations_arr + 0.5) / n_bins\n",
    "    dr_arr = radius * torch.sin(orientation_bin_midpoints)\n",
    "    dc_arr = radius * torch.cos(orientation_bin_midpoints)\n",
    "    hog_image = torch.zeros((image_len, image_len), dtype=torch.float)\n",
    "\n",
    "    for r in range(n_cells):\n",
    "        for c in range(n_cells):\n",
    "            for o, dr, dc in zip(orientations_arr, dr_arr, dc_arr):\n",
    "                center = tuple([r * cell_len + cell_len // 2, c * cell_len + cell_len // 2])\n",
    "                rr, cc = line(\n",
    "                    int(center[0] - dc),\n",
    "                    int(center[1] + dr),\n",
    "                    int(center[0] + dc),\n",
    "                    int(center[1] - dr),\n",
    "                )\n",
    "                hog_image[rr, cc] += histograms[r, c, o]\n",
    "\n",
    "    return hog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"img3.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "image = transform(img).to(device).unsqueeze(0)\n",
    "\n",
    "# Compute gradients\n",
    "grad_x, grad_y = compute_gradients(image)\n",
    "\n",
    "# Compute gradient magnitude and orientation\n",
    "grad = torch.sqrt(grad_x ** 2 + grad_y ** 2)\n",
    "theta = torch.atan2(grad_y, grad_x)\n",
    "\n",
    "grad = grad / grad.max()\n",
    "\n",
    "theta = theta * 180.0 / np.pi\n",
    "theta[theta < 0] += 180\n",
    "theta = torch.remainder(theta, 180.0)\n",
    "\n",
    "histograms = get_histograms(grad, theta)\n",
    "\n",
    "hog_image = hog(histograms.cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Histogram of Oriented Gradients\")\n",
    "plt.imshow(hog_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Invariant Feature Transform (SIFT)\n",
    "1. Scale-Space Extrema Detection - Search over multiple scales and image locations\n",
    "    - Finding blobs by approximating LoG with DoG\n",
    "    - Obtain gaussian pyramid for each layer in an octabe blur with different sigms for DoG\n",
    "2. Key-Point Localization - Fit a model to determine location and scale, then select key-points based on a measure of stability\n",
    "3. Orientation Assignment - Compute best orientations for each key-point region\n",
    "4. Key-Point Description - Use local image gradients at selected scale and rotation to describe each key-point region\n",
    "\n",
    "Automatic Scale Selection\n",
    "- Function responses for increasing scale (map different scales to the function results and the highest function result has the best scale)\n",
    "- Function can be 2nd derivative of Gaussian (Laplacian of Gaussian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
